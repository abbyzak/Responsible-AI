# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17ZsyT_wmIo33CkrCh3OCG1tG3UpQnUTG
"""

def classtolabel(ida):
  data = {
      0: "tench",
1: "goldfish",
2: "great_white_shark",
3: "tiger_shark",
4: "hammerhead",
5: "electric_ray",
6: "stingray",
7: "cock",
8: "hen"
  }
  return data[ida]

import tensorflow as tf
import numpy as np
import PIL.Image
from matplotlib import pylab as P

# Boilerplate methods.
def ShowImage(im, title='', ax=None):
  if ax is None:
    P.figure()
  P.axis('off')
  P.imshow(im)
  P.title(title)

def ShowGrayscaleImage(im, title='', ax=None):
  if ax is None:
    P.figure()
  P.axis('off')

  P.imshow(im, cmap=P.cm.gray, vmin=0, vmax=1)
  P.title(title)

def ShowHeatMap(im, title, ax=None):
  if ax is None:
    P.figure()
  P.axis('off')
  P.imshow(im, cmap='inferno')
  P.title(title)

def LoadImage(file_path):
  im = PIL.Image.open(file_path)
  im = im.resize((224,224))
  im = np.asarray(im)
  return im

def PreprocessImage(im):
  im = tf.keras.applications.vgg16.preprocess_input(im)
  return im

!pip install saliency
import saliency

# Change the below line to load in a different model!
m = tf.keras.applications.vgg16.VGG16(weights='imagenet', include_top=True)
model = tf.keras.models.Model([m.inputs], [m.output])
class_idx_str = 'class_idx_str'
def call_model_function(images, call_model_args=None, expected_keys=None):
    target_class_idx =  call_model_args[class_idx_str]
    images = tf.convert_to_tensor(images)
    with tf.GradientTape() as tape:
        if expected_keys==[saliency.base.INPUT_OUTPUT_GRADIENTS]:
            tape.watch(images)
            output_layer = model(images)
            output_layer = output_layer[:,target_class_idx]
            gradients = np.array(tape.gradient(output_layer, images))
            return {saliency.base.INPUT_OUTPUT_GRADIENTS: gradients}
        else:
            conv_layer, output_layer = model(images)
            gradients = np.array(tape.gradient(output_layer, conv_layer))
            return {saliency.base.CONVOLUTION_LAYER_VALUES: conv_layer,
                    saliency.base.CONVOLUTION_OUTPUT_GRADIENTS: gradients}

# Load the image
im_orig = LoadImage('./pic1.jpeg')
im = PreprocessImage(im_orig)

# Show the image
ShowImage(im_orig)

predictions = model(np.array([im]))
prediction_class = np.argmax(predictions[0])
call_model_args = {class_idx_str: prediction_class}

print("Prediction class: " + str(prediction_class))
print("Prediction class Name : " + classtolabel(prediction_class))

print(classtolabel(120))

!pip install mxnet

import torch
import torchvision.models as models

# ResNet50
resnet50 = models.resnet50(pretrained=True)

# VGG16
vgg16 = models.vgg16(pretrained=True)

# MobileNetV2
mobilenetv2 = models.mobilenet_v2(pretrained=True)

# InceptionV3
inceptionv3 = models.inception_v3(pretrained=True)
!pip install --upgrade efficientnet-pytorch

# EfficientNetB7
from efficientnet_pytorch import EfficientNet
efficientnetb7 = EfficientNet.from_pretrained('efficientnet-b7')

# Print out the models
print(resnet50)
print(vgg16)
print(mobilenetv2)
print(inceptionv3)
print(efficientnetb7)



"""PART 1"""

import torch
import torchvision.transforms as transforms
import numpy as np
from PIL import Image

# Define the image path
image_path = './pic1.jpeg'

# Define the transformation pipeline for each model
resnet50_transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225])
])

vgg16_transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225])
])

mobilenetv2_transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225])
])

inceptionv3_transform = transforms.Compose([
    transforms.Resize(299),
    transforms.CenterCrop(299),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.5, 0.5, 0.5],
        std=[0.5, 0.5, 0.5])
])

efficientnetb7_transform = transforms.Compose([
    transforms.Resize(600),
    transforms.CenterCrop(600),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.5, 0.5, 0.5],
        std=[0.5, 0.5, 0.5])
])

# Load the image and apply the transformations
image = Image.open(image_path).convert('RGB')

resnet50_image = resnet50_transform(image).unsqueeze(0)
vgg16_image = vgg16_transform(image).unsqueeze(0)
mobilenetv2_image = mobilenetv2_transform(image).unsqueeze(0)
inceptionv3_image = inceptionv3_transform(image).unsqueeze(0)
efficientnetb7_image = efficientnetb7_transform(image).unsqueeze(0)

# Define the models
resnet50 = models.resnet50(pretrained=True)
vgg16 = models.vgg16(pretrained=True)
mobilenetv2 = models.mobilenet_v2(pretrained=True)
inceptionv3 = models.inception_v3(pretrained=True)
efficientnetb7 = EfficientNet.from_pretrained('efficientnet-b7')

# Run predictions
resnet50_predictions = resnet50(resnet50_image)
resnet50_prediction_class = np.argmax(resnet50_predictions.detach().numpy())
print("ResNet50 Prediction class: " + str(resnet50_prediction_class))

vgg16_predictions = vgg16(vgg16_image)
vgg16_prediction_class = np.argmax(vgg16_predictions.detach().numpy())
print("VGG16 Prediction class: " + str(vgg16_prediction_class))

mobilenetv2_predictions = mobilenetv2(mobilenetv2_image)
mobilenetv2_prediction_class = np.argmax(mobilenetv2_predictions.detach().numpy())
print("MobileNetV2 Prediction class: " + str(mobilenetv2_prediction_class))

import torch
import torchvision.transforms as transforms
from efficientnet_pytorch import EfficientNet
from PIL import Image

# Define the image path
image_path = './pic1.jpeg'

# Define the transformation pipeline for EfficientNetB7
efficientnetb7_transform = transforms.Compose([
    transforms.Resize(600),
    transforms.CenterCrop(600),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.5, 0.5, 0.5],
        std=[0.5, 0.5, 0.5])
])

# Load the image and apply the transformations
image = Image.open(image_path).convert('RGB')

inceptionv3_image = inceptionv3_transform(image).unsqueeze(0)
efficientnetb7_image = efficientnetb7_transform(image).unsqueeze(0)

# Define the models
inceptionv3 = models.inception_v3(pretrained=True)
efficientnetb7 = EfficientNet.from_pretrained('efficientnet-b7')

# Run predictions for EfficientNetB7
efficientnetb7_predictions = efficientnetb7(efficientnetb7_image)
efficientnetb7_prediction_class = torch.argmax(efficientnetb7_predictions.detach(), dim=1).item()
print("EfficientNetB7 Prediction class: " + str(efficientnetb7_prediction_class))

transform = transforms.Compose([
    transforms.Resize(299),
    transforms.CenterCrop(299),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.5, 0.5, 0.5],
        std=[0.5, 0.5, 0.5])
])

# Load the image and apply the transformations
image = Image.open('./pic1.jpeg').convert('RGB')
image_tensor = transform(image).unsqueeze(0)

# Define the model
model = torch.hub.load('pytorch/vision:v0.9.0', 'inception_v3', pretrained=True)

# Run predictions
with torch.no_grad():
    model.eval()
    predictions = model(image_tensor)
    prediction_class = torch.argmax(predictions, dim=1).item()

print("Prediction class: " + str(prediction_class))

"""PART 2 Concensus"""

import torch
import torchvision.transforms as transforms
import urllib.request
from PIL import Image
import numpy as np


# Define the image file paths
image_paths = ['./pic1.jpeg']#, './pic2.jpeg', './pic3.jpeg', './pic4.jpeg', './pic5.jpeg']


# Define the transformation pipeline
transform = transforms.Compose([
    transforms.Resize(299),
    transforms.CenterCrop(299),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.5, 0.5, 0.5],
        std=[0.5, 0.5, 0.5])
])


# Define the models
models = [
    torch.hub.load('pytorch/vision:v0.9.0', 'resnet50', pretrained=True),
    torch.hub.load('pytorch/vision:v0.9.0', 'alexnet', pretrained=True),
    torch.hub.load('pytorch/vision:v0.9.0', 'vgg16', pretrained=True),
    torch.hub.load('pytorch/vision:v0.9.0', 'densenet121', pretrained=True),
    torch.hub.load('pytorch/vision:v0.9.0', 'inception_v3', pretrained=True)
]
# Load the image and apply the transformations
#open image like this
#image = Image.open('./pic1.jpeg').convert('RGB')
for image in image_paths:
  image = Image.open(image).convert('RGB')
  image_tensor = transform(image).unsqueeze(0)
  # Run predictions for each model
  predictions = []
  for model in models:
      with torch.no_grad():
          model.eval()
          output = model(image_tensor)
          predictions.append(output.numpy()[0])

  # Compute the consensus prediction
  consensus = np.mean(predictions, axis=0)
  prediction_class = np.argmax(consensus)

  print("Consensus prediction class: " + str(prediction_class))
  print("Naeme of predicted class: " + classtolabel(prediction_class))

"""Part 3
To fool one of the models in the ensemble, I will choose the VGG16 model. I will experiment with two images of dogs, which are included in the ImageNet training set, to see if I can make the model misclassify them. Specifically, I will use the following two images:

A close-up of a dog's face: https://www.pexels.com/photo/brown-short-coated-dog-with-tongue-out-4165445/
A photo of a dog playing with a ball: https://www.pexels.com/photo/dog-running-on-green-grass-field-2777898/
For the first image, I will use an adversarial attack that adds a small perturbation to the image to make the model misclassify it. For the second image, I will apply a beauty filter to the image to see if the model can be fooled by the transformed image.

First, I will load the VGG16 model and the two images:
**bold text**
"""

import urllib.request
import tensorflow as tf
import numpy as np
from PIL import Image

# Define the image file URLs
image_urls = ['https://cdn.pixabay.com/photo/2018/01/14/23/12/nature-3082832_960_720.jpg',
              'https://cdn.pixabay.com/photo/2018/02/08/22/27/flower-3140492_960_720.jpg',
              'https://cdn.pixabay.com/photo/2015/11/06/13/29/kitten-1028546_960_720.jpg']

# Download the images
for i, url in enumerate(image_urls):
    urllib.request.urlretrieve(url, f"./image_{i+1}.jpeg")

# Load the ResNet50 model
model = tf.keras.applications.ResNet50(weights='imagenet')

# Define the FGSM function
def create_adversarial_pattern(input_image, input_label, model):
    loss_object = tf.keras.losses.CategoricalCrossentropy()

    with tf.GradientTape() as tape:
        tape.watch(input_image)
        prediction = model(input_image)
        loss = loss_object(input_label, prediction)

    gradient = tape.gradient(loss, input_image)
    signed_grad = tf.sign(gradient)
    return signed_grad

# Define the targeted class (i.e., the class we want the model to misclassify as)
target_class = 25  # this is the "badger" class in ImageNet

# Load the test images and generate adversarial perturbations for each one
for i in range(1, 4):
    # Load the image
    im_orig = Image.open(f'./image_{i}.jpeg')
    im = np.array(im_orig)

    # Preprocess the image
    x = tf.keras.applications.resnet50.preprocess_input(im)

    # Create a one-hot encoding of the target class
    target = np.zeros((1, 1000))
    target[:, target_class] = 1

    # Generate the adversarial perturbation
    perturbation = create_adversarial_pattern(tf.convert_to_tensor([x]), tf.convert_to_tensor(target), model)

    # Apply the perturbation to the image
    x_adv = x + 0.1 * perturbation.numpy()

    # Get the model's predictions for the original and perturbed images
    preds_orig = model.predict(np.array([x]))
    preds_adv = model.predict(np.array([x_adv]))

    # Print the top 5 predicted classes and their probabilities for each image
    print(f"Original image {i} predictions:")
    for j in range(5):
        print(f"{j+1}. {tf.keras.applications.resnet50.decode_predictions(preds_orig, top=5)[0][j]}")
    print(f"Perturbed image {i} predictions:")
    for j in range(5):
        print(f"{j+1}. {tf.keras.applications.resnet50.decode_predictions(preds_adv, top=5)[0][j]}")

import torch
from torchvision.models import vgg16
from PIL import Image
from torch.nn import functional as F
import numpy as np
import matplotlib.pyplot as plt

# Load the VGG16 model
model = vgg16(pretrained=True)
model.eval()

# Define the SmoothGrad function
def smoothgrad(model, x, n_samples=30, std=0.1):
    x = x.unsqueeze(0)
    saliency = torch.zeros_like(x)
    for _ in range(n_samples):
        noise = torch.randn_like(x) * std
        x_noisy = x + noise
        out = model(x_noisy)
        out[0, out.argmax()].backward()
        saliency += x_noisy
    saliency /= n_samples
    saliency = saliency.squeeze().detach().numpy()
    saliency = np.transpose(saliency, (1, 2, 0))
    saliency -= saliency.min()
    saliency /= saliency.max()
    return saliency

# Load the images and generate the saliency maps
images = ['./pic1.jpeg']#, 'pic2.jpeg', 'pic3.jpeg', 'pic4.jpeg', 'pic5.jpeg']

for image_file in images:
    # Load the image
    img = Image.open(image_file)
    
    # Preprocess the image
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    x = transform(img)
    
    # Generate the saliency map
    saliency = smoothgrad(model, x)
    
    # Show the original image and the saliency map
    plt.figure()
    plt.subplot(1, 2, 1)
    plt.imshow(img)
    plt.axis('off')
    plt.subplot(1, 2, 2)
    plt.imshow(saliency, cmap='jet')
    plt.axis('off')
    plt.show()



"""LAST PART

The field of AI has made significant progress in recent years, but it also faces several challenges that must be addressed to ensure that AI systems are responsible and trustworthy. One important challenge is bias in AI systems. AI systems are trained on large datasets, and if these datasets contain biased data, the AI system will also be biased. This can have serious consequences, such as perpetuating discrimination and reinforcing stereotypes. To tackle this challenge, researchers have proposed several approaches such as data augmentation, data balancing, and fairness constraints to reduce bias in AI systems.

Another challenge in AI is the lack of interpretability and explainability of AI systems. It is essential to understand why an AI system has made a particular decision, especially in high-stake applications such as healthcare and finance. Researchers have proposed several methods such as saliency maps, SHAP values, and LIME to provide interpretability and explainability to AI systems. However, there is still a lot of work to be done in this area to ensure that AI systems can provide interpretable and trustworthy decisions.

A related challenge is the lack of transparency in AI systems. AI systems are often considered black boxes, where the input goes in, and the output comes out, without any insight into the internal workings of the system. This lack of transparency makes it challenging to understand how an AI system arrives at a decision and can lead to mistrust and suspicion of AI systems. To address this challenge, researchers have proposed several approaches such as model introspection, model distillation, and hybrid intelligence, which combines the strengths of human intelligence and machine intelligence.

Another challenge in AI is the lack of privacy of AI systems. AI systems often require access to sensitive data such as healthcare records and financial data. This data can be subject to data breaches, privacy violations, and misuse. To address this challenge, researchers have proposed several approaches such as privacy-preserving machine learning, federated learning, and differential privacy, which provide privacy guarantees while ensuring that the AI system can learn from the data.

In conclusion, the responsible development and deployment of AI systems require addressing several challenges such as bias, interpretability, transparency, and privacy. The AI community is actively working to address these challenges by proposing new methods and techniques to make AI systems more responsible and trustworthy. However, there is still much work to be done to ensure that AI systems are developed and deployed responsibly and ethically.
"""

